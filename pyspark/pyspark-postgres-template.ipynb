{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose\n",
        "\n",
        "Explore PySpark and the JDBC connection functionality to read from operational databases.\n",
        "\n",
        "In this notebook we will setup a PostgreSQL instance and populate it with the Pagila dataset. We will then connect to the database via a JDBC connector."
      ],
      "metadata": {
        "id": "8QbPsmEf6ljt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "## PostgreSQL\n",
        "\n",
        "Firstly, let's install postgres in the this Colab instance."
      ],
      "metadata": {
        "id": "f-RHL4bg4u0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install postgresql postgresql-contrib "
      ],
      "metadata": {
        "id": "qhmGVh22JcNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!service postgresql start"
      ],
      "metadata": {
        "id": "ajhL0Z_-KK8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a user in Postgres ([stackoverflow](https://stackoverflow.com/questions/12720967/how-to-change-postgresql-user-password/12721020#12721020))\n"
      ],
      "metadata": {
        "id": "_P48P8Vt6Fm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'test';\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25UVuzVNdKs",
        "outputId": "347729a8-6e4a-455d-9a1f-334b3a7d6e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALTER ROLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store you database password in an environmental variable so that we need no type it in all the time (not advisable generally).\n",
        "\n",
        "We'll use the notebook magic `%end`"
      ],
      "metadata": {
        "id": "JW1kucySWAKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env PGPASSWORD=test"
      ],
      "metadata": {
        "id": "as0Zs9kL6PY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pagila\n",
        "\n",
        "Now, let's populate the PostgreSQL database with the Pagila data from the tutorial."
      ],
      "metadata": {
        "id": "LGqYbg366efu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/spatialedge-ai/pagila.git"
      ],
      "metadata": {
        "id": "qICjoP_dKS8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!psql -h localhost -U postgres -c \"create database pagila\""
      ],
      "metadata": {
        "id": "xYHVKYqSMthy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-schema.sql\""
      ],
      "metadata": {
        "id": "kfgNogz3MSq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-data.sql\""
      ],
      "metadata": {
        "id": "8zpqaYNZPABo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Setup\n",
        "\n",
        "Now, let's download what is necessary for initiating jdbc connections, as well as what is required to run PySpark itself."
      ],
      "metadata": {
        "id": "9M0a4GiI6yyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/34948296/using-pyspark-to-connect-to-postgresql\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.5.0.jar"
      ],
      "metadata": {
        "id": "bCiCzTg-Jx2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np  \n",
        "\n",
        "%config Completer.use_jedi = False\n",
        "\n",
        "\n",
        "SPARKVERSION='2.4.8'\n",
        "HADOOPVERSION='2.7'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}\"\n",
        "\n",
        "print(os.environ['SPARK_HOME'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BQsxrwZBhWc",
        "outputId": "9eb5c526-123c-4e27-daff-a41498fedbcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-2.4.8-bin-hadoop2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://archive.apache.org/dist/spark/spark-{SPARKVERSION}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz\n",
        "!tar xf spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz"
      ],
      "metadata": {
        "id": "1owkTgHVBuix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp postgresql-42.5.0.jar spark-2.4.8-bin-hadoop2.7/jars"
      ],
      "metadata": {
        "id": "Ighjc_WdUNgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCIQhdSYC5uh",
        "outputId": "e7ebaed9-9784-4092-f550-beb37e567825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# get a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.config(\"spark.jars\", \n",
        "                                                       \"postgresql-42.2.5.jar\").config(\n",
        "                                                          \"spark.driver.extraClassPath\",\n",
        "                                                          \"spark-2.4.8-bin-hadoop2.7/jars\"\n",
        "                                                       ).getOrCreate()\n",
        "print(spark.conf.get('spark.jars'))\n",
        "\n",
        "%env PYARROW_IGNORE_TIMEZONE=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reMhwdxpCz05",
        "outputId": "d3c11ced-e4c9-4100-9759-ba022f2bfd68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYARROW_IGNORE_TIMEZONE=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "\n",
        "### Question 1\n",
        "\n",
        "Using a PySpark dataframe, print the schema of customer table in the pagila PostgreSQL database by utilising a JDBC connection."
      ],
      "metadata": {
        "id": "IqG_Hk4YXuC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark code"
      ],
      "metadata": {
        "id": "EnrQk09jQyaJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "\n",
        "Use the Spark SQL API to query the customer table, compute the number of unique email addresses in that table and print the result in the notebook."
      ],
      "metadata": {
        "id": "tXhnjaylCFI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark code"
      ],
      "metadata": {
        "id": "xTGwAFhYpanl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3 \n",
        "\n",
        "Repeat this calculation using only the Dataframe API and print the result."
      ],
      "metadata": {
        "id": "bg7To_5dCRGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark code"
      ],
      "metadata": {
        "id": "hTO78anmCa37"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4 \n",
        "\n",
        "How many partitions are present in the dataframe resulting from Question 3 (additionally provide the code necessary to determine that)"
      ],
      "metadata": {
        "id": "8IIL4RDSCcn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5\n",
        "\n",
        "Compute the min and max of customer.create_date and print the result (once more using the Spark DataFrame API and not the Spark SQL API)."
      ],
      "metadata": {
        "id": "P_6o4oLIC5SJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6.1\n",
        "\n",
        "Determine which first names occur more than once:\n",
        "\n",
        "1. using the Spark SQL API (printing the result)"
      ],
      "metadata": {
        "id": "8vndZmoyC-Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6.2\n",
        "\n",
        "  2. using the Spark Dataframe API (printing the result once more)."
      ],
      "metadata": {
        "id": "d-qGmjBqDErO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7\n",
        "\n",
        "Port the PostgreSQL below to the PySpark DataFrame API and execute the query within Spark (not directly on PostgreSQL): \n",
        "\n",
        "```\n",
        "SELECT\n",
        "   staff.first_name\n",
        "   ,staff.last_name\n",
        "   ,SUM(payment.amount)\n",
        " FROM payment\n",
        "   INNER JOIN staff ON payment.staff_id = staff.staff_id\n",
        " WHERE payment.payment_date BETWEEN '2007-01-01' AND '2007-02-01'\n",
        " GROUP BY\n",
        "   staff.last_name\n",
        "   ,staff.first_name\n",
        " ORDER BY SUM(payment.amount)\n",
        " ;\n",
        "```"
      ],
      "metadata": {
        "id": "qA56WFXXDqrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 8\n",
        "\n",
        "Are you currently executing commands on a driver node, or a worker? Provide the code you ran to determine that."
      ],
      "metadata": {
        "id": "Qqv7FoidJiBJ"
      }
    }
  ]
}