{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QbPsmEf6ljt"
   },
   "source": [
    "# Purpose\n",
    "\n",
    "Explore PySpark and the JDBC connection functionality to read from operational databases.\n",
    "\n",
    "In this notebook we will setup a PostgreSQL instance and populate it with the Pagila dataset. We will then connect to the database via a JDBC connector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-RHL4bg4u0_"
   },
   "source": [
    "# Setup\n",
    "\n",
    "## PostgreSQL\n",
    "\n",
    "Firstly, let's install postgres in the this Colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhmGVh22JcNo",
    "outputId": "a6492cbc-2798-4f2d-e942-398dc9f68dc1"
   },
   "outputs": [],
   "source": [
    "!sudo apt install postgresql postgresql-contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajhL0Z_-KK8r",
    "outputId": "72539cf6-ccbf-49f6-a997-849941a52e65"
   },
   "outputs": [],
   "source": [
    "!service postgresql start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P48P8Vt6Fm9"
   },
   "source": [
    "Create a user in Postgres ([stackoverflow](https://stackoverflow.com/questions/12720967/how-to-change-postgresql-user-password/12721020#12721020))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b25UVuzVNdKs",
    "outputId": "8771887b-3f3d-4817-d19d-8c9fbf766a31"
   },
   "outputs": [],
   "source": [
    "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'test';\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW1kucySWAKv"
   },
   "source": [
    "Store you database password in an environmental variable so that we need no type it in all the time (not advisable generally).\n",
    "\n",
    "We'll use the notebook magic `%end`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "as0Zs9kL6PY0",
    "outputId": "e890d4d9-d91c-4812-e0aa-5843f7ef0a8a"
   },
   "outputs": [],
   "source": [
    "%env PGPASSWORD=test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGqYbg366efu"
   },
   "source": [
    "## Pagila\n",
    "\n",
    "Now, let's populate the PostgreSQL database with the Pagila data from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qICjoP_dKS8G",
    "outputId": "38fd2dc4-5f7d-4893-c2e8-4dbc1661d133"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/spatialedge-ai/pagila.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYHVKYqSMthy",
    "outputId": "38859490-60e9-47e5-cbd0-22e012dd5cd0"
   },
   "outputs": [],
   "source": [
    "!psql -h localhost -U postgres -c \"create database pagila\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfgNogz3MSq_",
    "outputId": "4133c128-ca32-4da4-fa77-01346421db4a"
   },
   "outputs": [],
   "source": [
    "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-schema.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zpqaYNZPABo",
    "outputId": "9f5ba9b5-1c53-4649-e6a1-a89a8acd2073"
   },
   "outputs": [],
   "source": [
    "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-data.sql\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M0a4GiI6yyr"
   },
   "source": [
    "## PySpark Setup\n",
    "\n",
    "Now, let's download what is necessary for initiating jdbc connections, as well as what is required to run PySpark itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCiCzTg-Jx2Q",
    "outputId": "5cef5276-0cce-4936-c0e6-01d3b211ea99"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34948296/using-pyspark-to-connect-to-postgresql\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.5.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BQsxrwZBhWc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "SPARKVERSION='3.2.1'\n",
    "HADOOPVERSION='3.2'\n",
    "pwd=os.getcwd()\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}\"\n",
    "\n",
    "# print(os.environ['SPARK_HOME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1owkTgHVBuix",
    "outputId": "7979e707-c026-49d1-acb8-5726431a3b3e"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://archive.apache.org/dist/spark/spark-{SPARKVERSION}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz\n",
    "!tar xf spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ighjc_WdUNgC"
   },
   "outputs": [],
   "source": [
    "!cp postgresql-42.5.0.jar spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8DvgX7OEHWo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCIQhdSYC5uh",
    "outputId": "4a8c5696-be4c-4846-b887-f206f8821789"
   },
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reMhwdxpCz05",
    "outputId": "52928e2d-135e-4206-c033-fd68c9dba321"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "# get a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.jars\",\n",
    "                                                       \"postgresql-42.2.5.jar\").config(\n",
    "                                                          \"spark.driver.extraClassPath\",\n",
    "                                                          f\"spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\"\n",
    "                                                       ).getOrCreate()\n",
    "print(spark.conf.get('spark.jars'))\n",
    "\n",
    "%env PYARROW_IGNORE_TIMEZONE=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqG_Hk4YXuC7"
   },
   "source": [
    "# Questions\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Using a PySpark dataframe, print the schema of customer table in the pagila PostgreSQL database by utilising a JDBC connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnrQk09jQyaJ"
   },
   "outputs": [],
   "source": [
    "# pyspark code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXhnjaylCFI1"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Use the Spark SQL API to query the customer table, compute the number of unique email addresses in that table and print the result in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTGwAFhYpanl"
   },
   "outputs": [],
   "source": [
    "# pyspark code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg7To_5dCRGb"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Repeat this calculation using only the Dataframe API and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTO78anmCa37"
   },
   "outputs": [],
   "source": [
    "# pyspark code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IIL4RDSCcn4"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "How many partitions are present in the dataframe resulting from Question 3 (additionally provide the code necessary to determine that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_6o4oLIC5SJ"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Compute the min and max of customer.create_date and print the result (once more using the Spark DataFrame API and not the Spark SQL API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vndZmoyC-Ay"
   },
   "source": [
    "### Question 6.1\n",
    "\n",
    "Determine which first names occur more than once:\n",
    "\n",
    "1. using the Spark SQL API (printing the result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-qGmjBqDErO"
   },
   "source": [
    "### Question 6.2\n",
    "\n",
    "  2. using the Spark Dataframe API (printing the result once more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA56WFXXDqrm"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Port the PostgreSQL below to the PySpark DataFrame API and execute the query within Spark (not directly on PostgreSQL):\n",
    "\n",
    "```\n",
    "SELECT\n",
    "   staff.first_name\n",
    "   ,staff.last_name\n",
    "   ,SUM(payment.amount)\n",
    " FROM payment\n",
    "   INNER JOIN staff ON payment.staff_id = staff.staff_id\n",
    " WHERE payment.payment_date BETWEEN '2007-01-01' AND '2007-02-01'\n",
    " GROUP BY\n",
    "   staff.last_name\n",
    "   ,staff.first_name\n",
    " ORDER BY SUM(payment.amount)\n",
    " ;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqv7FoidJiBJ"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "Are you currently executing commands on a driver node, or a worker? Provide the code you ran to determine that."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
